% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/concaveFDR.R
\name{concaveFDR}
\alias{concaveFDR}
\title{Estimate (Local) False Discovery Rates For Diverse Test Statistics
Using Log-Concave Densities}
\usage{
concaveFDR(x, statistic = c("normal", "correlation", "pvalue"), plot = TRUE,
  color.figure = TRUE, verbose = TRUE, theo = FALSE, scale_param = NULL,
  alternative = c("grenander", "log-concave"), cutoff.method = c("fndr",
  "pct0", "locfdr", "smoothing"), pct0 = 0.75)
}
\arguments{
\item{x}{vector of the observed test statistics.}

\item{statistic}{one of "normal" (default), "correlation", "pvalue".  This
species the null model.}

\item{plot}{plot a figure with estimated densities, distribution functions,
and (local) false discovery rates.}

\item{color.figure}{determines whether a color figure or a black and white
figure is produced (defaults to "TRUE", i.e. to color figure).}

\item{verbose}{print out status messages.}

\item{theo}{Should the theoretical null model be used? In this case no null
model scale parameters (e.g. sigma for z-scores) are estimated. Note that the df and
the kappa parameters for t-statistics and the correlation statistics have to
be set manually as they depeden on the original sample sizes.}

\item{scale_param}{scale parameter used when calculating the statistics, only needs to
be specified if \code{theo} is set to TRUE and the statistics are t-scores
or correlations. See \code{\link{dcor0}} for details on how to set this for correlations.
For t-statistics, this depedends on the type of t-test performed.}

\item{alternative}{The estimation algorithm used for estimation of the alternative
densitiy. "Grenander" corresponds to the method implemented in fdrtool, while
"log-concave" is new method introduced in the package}

\item{cutoff.method}{one of "fndr" (default), "pct0", "locfdr".}

\item{pct0}{fraction of data used for fitting null model - only if
\code{cutoff.method}="pct0"}
}
\value{
A list with the following components:
\item{ pval }{a vector with p-values for each case.}
\item{ qval}{a vector with q-values (Fdr) for each case..}
\item{ lfdr }{ a vector with local fdr values for each case.}
\item{ statistic }{the specified type of null model.}
\item{ param }{ a vector containing the estimated parameters (the null
proportion \code{eta0} and the free parameter of the null model).}
}
\description{
\code{concaveFDR} takes a vector of z-scores (or of correlations, p-values, or
t-statistics), and estimates for each case both the tail area-based Fdr as
well as the density-based fdr (=q-value resp. local false discovery rate).
The parameters of the null distribution are estimated adaptively from the
data (except for the case of p-values where this is not necessary).
}
\details{
The algorithm implemented in this function proceeds as follows:

\enumerate{ \item A suitable cutoff point is determined.  If
\code{cutoff.method} is "fndr" then first an approximate null model is
fitted and subsequently a cutoff point is sought with false nondiscovery
rate as small as possible (see \code{\link{fndr.cutoff}}).  If
\code{cutoff.method} is "pct0" then a specified quantile (default value:
0.75) of the data is used as the cutoff point.  If \code{cutoff.method}
equals "locfdr" then the heuristic of the "locfdr" package (version 1.1-6)
is employed to find the cutoff (z-scores and correlations only).
If \code{cutoff.method} is "smoothing" the a smoothed b-spline is
fitted to the eta0 estimates at various cutoff points and flat regions
are found by inspecting local minima of the smoothed b-spline curve.

\item The
parameters of the null model are estimated from the data using
\code{\link{censored.fit}}. This results in estimates for scale parameters
und and proportion of null values (\code{eta0}). Not that choosing theo = TRUE
will results in using the theoretical null model without a scale parameter.

\item Subsequently the
corresponding p-values are computed, and a modified \code{\link{grenander}} or
log-concave density estimation
algorithm is employed to obtain the overall density and distribution
function (note that this respects the estimated \code{eta0}). The choice
of the density estimation algorithm can be made by using the \code{alternative}
argument.

\item
Finally, q-values and local fdr values are computed for each case.  }

The assumed null models all have (except for p-values) one free scale
parameter.  Note that the z-scores and the correlations are assumed to have
zero mean.
}
\examples{

# load "fdrtool" library and p-values
library("fdrtool")
data(pvalues)


# estimate fdr and Fdr from p-values

data(pvalues)
fdr <- concaveFDR(pvalues, statistic="pvalue")
fdr$qval.log # estimated Fdr values using log-concave density estimation
fdr$qval.gr # estimated local fdr using Grenander density estimation

# the same but with black and white figure
fdr <- concaveFDR(pvalues, statistic="pvalue", color.figure=FALSE)


# estimate fdr and Fdr from z-scores

sd.true = 2.232
n = 500
z = rnorm(n, sd=sd.true)
z = c(z, runif(30, 5, 10)) # add some contamination
fdr =  concaveFDR(z)

# you may change some parameters of the underlying functions
fdr =  concaveFDR(z, cutoff.method="pct0", pct0=0.9)




}
\references{
Strimmer, K. (2008a).  A unified approach to false discovery rate
estimation. BMC Bioinformatics 9: 303.  Available from
\url{http://www.biomedcentral.com/1471-2105/9/303/}.

Strimmer, K. (2008b). fdrtool: a versatile R package for estimating local
and tail area- based false discovery rates.  Bioinformatics 24: 1461-1462.
Available from
\url{http://bioinformatics.oxfordjournals.org/cgi/content/abstract/24/12/1461}.
}
\seealso{
\code{\link{pval.estimate.eta0}}, \code{\link{censored.fit}}.
}
\author{
Bernd Klaus, Korbinian Strimmer (\url{http://strimmerlab.org}).
}
